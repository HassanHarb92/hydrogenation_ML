{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "851b5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  unsat_SMILE sat_SMILE     delta_H  nH2    pH2\n",
      "0         C#C        CC  150.735206    2  13.42\n",
      "1         C=O        CO   83.774454    1   6.29\n",
      "2        CC#C       CCC  139.811813    2   9.15\n",
      "3        CC=O       CCO   63.227291    1   4.38\n",
      "4     CC(C)=O    CC(C)O   51.916637    1   3.36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('gdb9_G4MP2_withdata_hydrogenation_clean.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29ec0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rdkit.Chem.Fragments as Fragments\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit.Chem.Crippen as Crippen\n",
    "import rdkit.Chem.Lipinski as Lipinski\n",
    "import rdkit.Chem.rdMolDescriptors as MolDescriptors\n",
    "import rdkit.Chem.Descriptors as Descriptors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C ,WhiteKernel as Wht,Matern as matk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfb19bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training data size: 970\n",
      "Unlabeled pool size: 18436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 5% for initial training, 95% as unlabeled pool\n",
    "initial_data, unlabeled_pool = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Initial training data size:\", initial_data.shape[0])\n",
    "print(\"Unlabeled pool size:\", unlabeled_pool.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa38ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_chem_mol(mol):\n",
    "    mol_sssr = Chem.GetSSSR(mol)\n",
    "    clogp    = Crippen.MolLogP(mol)\n",
    "    mr       = Crippen.MolMR(mol)\n",
    "    mw       = MolDescriptors.CalcExactMolWt(mol)\n",
    "    tpsa    = MolDescriptors.CalcTPSA(mol)\n",
    "    Chi0n    = MolDescriptors.CalcChi0n(mol)\n",
    "    Chi1n    = MolDescriptors.CalcChi1n(mol)\n",
    "    Chi2n    = MolDescriptors.CalcChi2n(mol)\n",
    "    Chi3n    = MolDescriptors.CalcChi3n(mol)\n",
    "    Chi4n    = MolDescriptors.CalcChi4n(mol)\n",
    "    Chi0v    = MolDescriptors.CalcChi0v(mol)\n",
    "    Chi1v    = MolDescriptors.CalcChi1v(mol)\n",
    "    Chi2v    = MolDescriptors.CalcChi2v(mol)\n",
    "    Chi3v    = MolDescriptors.CalcChi3v(mol)\n",
    "    Chi4v    = MolDescriptors.CalcChi4v(mol)\n",
    "    fracsp3  = MolDescriptors.CalcFractionCSP3(mol)\n",
    "    Hall_Kier_Alpha = MolDescriptors.CalcHallKierAlpha(mol)\n",
    "    Kappa1      = MolDescriptors.CalcKappa1(mol)\n",
    "    Kappa2      = MolDescriptors.CalcKappa2(mol)\n",
    "    Kappa3      = MolDescriptors.CalcKappa3(mol)\n",
    "    LabuteASA   = MolDescriptors.CalcLabuteASA(mol)\n",
    "    Number_Aliphatic_Rings = MolDescriptors.CalcNumAliphaticRings(mol)\n",
    "    Number_Aromatic_Rings = MolDescriptors.CalcNumAromaticRings(mol)\n",
    "    Number_Amide_Bonds = MolDescriptors.CalcNumAmideBonds(mol)\n",
    "    Number_Atom_Stereocenters = MolDescriptors.CalcNumAtomStereoCenters(mol)\n",
    "    Number_BridgeHead_Atoms = MolDescriptors.CalcNumBridgeheadAtoms(mol)\n",
    "    Number_HBA = MolDescriptors.CalcNumHBA(mol)\n",
    "    Number_HBD = MolDescriptors.CalcNumHBD(mol)\n",
    "    Number_Hetero_Atoms = MolDescriptors.CalcNumHeteroatoms(mol)\n",
    "    Number_Hetero_Cycles = MolDescriptors.CalcNumHeterocycles(mol)\n",
    "    Number_Rings = MolDescriptors.CalcNumRings(mol)\n",
    "    Number_Rotatable_Bonds = MolDescriptors.CalcNumRotatableBonds(mol)\n",
    "    Number_Spiro = MolDescriptors.CalcNumSpiroAtoms(mol)\n",
    "    Number_Saturated_Rings = MolDescriptors.CalcNumSaturatedRings(mol)\n",
    "    Number_Heavy_Atoms = Lipinski.HeavyAtomCount(mol)\n",
    "    Number_NH_OH = Lipinski.NHOHCount(mol)\n",
    "    Number_N_O = Lipinski.NOCount(mol)\n",
    "    Number_Valence_Electrons = Descriptors.NumValenceElectrons(mol)\n",
    "    Max_Partial_Charge = Descriptors.MaxPartialCharge(mol)\n",
    "    Min_Partial_Charge = Descriptors.MinPartialCharge(mol)\n",
    "\n",
    "    return mol_sssr, clogp, mr, mw, tpsa, Chi0n, Chi1n, Chi2n, Chi3n, Chi4n, Chi0v, Chi1v, Chi2v, Chi3v, Chi4v, fracsp3, Hall_Kier_Alpha,Kappa1, Kappa2, Kappa3, LabuteASA, Number_Aliphatic_Rings, Number_Aromatic_Rings, Number_Amide_Bonds, Number_Atom_Stereocenters, Number_BridgeHead_Atoms, Number_HBA, Number_HBD, Number_Hetero_Atoms, Number_Hetero_Cycles, Number_Rings, Number_Rotatable_Bonds, Number_Spiro, Number_Saturated_Rings, Number_Heavy_Atoms, Number_NH_OH, Number_N_O, Number_Valence_Electrons, Max_Partial_Charge, Min_Partial_Charge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8262b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_descriptors_for_dataframe(df, smiles_column):\n",
    "    # Initialize lists to hold descriptors\n",
    "    descriptors = []\n",
    "\n",
    "    # Loop through SMILES strings in the dataframe\n",
    "    for smiles in df[smiles_column]:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:  # check if mol conversion is successful\n",
    "            descriptors.append(evaluate_chem_mol(mol))\n",
    "        else:\n",
    "            # Append None or NaN for molecules that fail conversion\n",
    "            descriptors.append([None]*40)  # 40 being the number of descriptors\n",
    "\n",
    "    # Convert list of descriptors to DataFrame\n",
    "    desc_df = pd.DataFrame(descriptors, columns=[\n",
    "        'mol_sssr', 'clogp', 'mr', 'mw', 'tpsa', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n', \n",
    "        'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v', 'fracsp3', 'Hall_Kier_Alpha','Kappa1', \n",
    "        'Kappa2', 'Kappa3', 'LabuteASA', 'Number_Aliphatic_Rings', 'Number_Aromatic_Rings', \n",
    "        'Number_Amide_Bonds', 'Number_Atom_Stereocenters', 'Number_BridgeHead_Atoms', 'Number_HBA', \n",
    "        'Number_HBD', 'Number_Hetero_Atoms', 'Number_Hetero_Cycles', 'Number_Rings', 'Number_Rotatable_Bonds', \n",
    "        'Number_Spiro', 'Number_Saturated_Rings', 'Number_Heavy_Atoms', 'Number_NH_OH', 'Number_N_O', \n",
    "        'Number_Valence_Electrons', 'Max_Partial_Charge', 'Min_Partial_Charge'\n",
    "    ])\n",
    "    \n",
    "    return pd.concat([df, desc_df], axis=1)\n",
    "\n",
    "# Compute descriptors for the entire dataset\n",
    "df = compute_descriptors_for_dataframe(df, 'unsat_SMILE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e02ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "# Split data into initial training set and unlabeled pool\n",
    "initial_idx = np.random.choice(range(len(df)), size=500, replace=False)\n",
    "pool_idx = [i for i in range(len(df)) if i not in initial_idx]\n",
    "\n",
    "X = df.iloc[:, 5:].values  # Assuming descriptors start from the 5th column\n",
    "y = df['delta_H'].values\n",
    "\n",
    "X_initial = X[initial_idx]\n",
    "y_initial = y[initial_idx]\n",
    "X_pool = X[pool_idx]\n",
    "y_pool = y[pool_idx]\n",
    "\n",
    "# Initialize Gaussian Process\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gp = GaussianProcessRegressor(kernel=kernel, random_state=1)\n",
    "\n",
    "# Train on the initial data\n",
    "gp.fit(X_initial, y_initial)\n",
    "\n",
    "# Make predictions on the pool\n",
    "predictions, std_dev = gp.predict(X_pool, return_std=True)\n",
    "\n",
    "# Let's say we want to add 100 least confident points to our training data\n",
    "uncertainty_idx = np.argsort(std_dev)[-100:]\n",
    "\n",
    "# Append the uncertain points to the initial data\n",
    "X_initial = np.vstack([X_initial, X_pool[uncertainty_idx]])\n",
    "y_initial = np.hstack([y_initial, y_pool[uncertainty_idx]])\n",
    "\n",
    "# Remove the uncertain points from the pool\n",
    "X_pool = np.delete(X_pool, uncertainty_idx, axis=0)\n",
    "y_pool = np.delete(y_pool, uncertainty_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac1699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 complete. Training set size: 1000 | Test MSE: 967.2802\n",
      "Iteration 2/50 complete. Training set size: 1400 | Test MSE: 956.0414\n",
      "Iteration 3/50 complete. Training set size: 1800 | Test MSE: 920.9549\n",
      "Iteration 4/50 complete. Training set size: 2200 | Test MSE: 898.9222\n",
      "Iteration 5/50 complete. Training set size: 2600 | Test MSE: 859.9096\n",
      "Iteration 6/50 complete. Training set size: 3000 | Test MSE: 845.8091\n",
      "Iteration 7/50 complete. Training set size: 3400 | Test MSE: 821.0274\n",
      "Iteration 8/50 complete. Training set size: 3800 | Test MSE: 798.2690\n",
      "Iteration 9/50 complete. Training set size: 4200 | Test MSE: 782.3044\n",
      "Iteration 10/50 complete. Training set size: 4600 | Test MSE: 768.7704\n",
      "Iteration 11/50 complete. Training set size: 5000 | Test MSE: 754.6201\n",
      "Iteration 12/50 complete. Training set size: 5400 | Test MSE: 740.5028\n",
      "Iteration 13/50 complete. Training set size: 5800 | Test MSE: 731.4334\n",
      "Iteration 14/50 complete. Training set size: 6200 | Test MSE: 719.1032\n",
      "Iteration 15/50 complete. Training set size: 6600 | Test MSE: 713.7537\n",
      "Iteration 16/50 complete. Training set size: 7000 | Test MSE: 705.5652\n",
      "Iteration 17/50 complete. Training set size: 7400 | Test MSE: 697.8816\n",
      "Iteration 18/50 complete. Training set size: 7800 | Test MSE: 690.5169\n",
      "Iteration 19/50 complete. Training set size: 8200 | Test MSE: 684.5604\n",
      "Iteration 20/50 complete. Training set size: 8600 | Test MSE: 678.4362\n",
      "Iteration 21/50 complete. Training set size: 9000 | Test MSE: 674.1407\n",
      "Iteration 22/50 complete. Training set size: 9400 | Test MSE: 671.0402\n",
      "Iteration 23/50 complete. Training set size: 9800 | Test MSE: 666.0919\n",
      "Iteration 24/50 complete. Training set size: 10200 | Test MSE: 661.9576\n",
      "Iteration 25/50 complete. Training set size: 10600 | Test MSE: 657.4957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Before active learning loop, split out a small test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Modify your existing loop to also record performance on the test set\n",
    "mse_test = []\n",
    "\n",
    "n_iterations = 50  # Existing value\n",
    "n_to_add_each_iter = 400  # Existing value\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Existing code...\n",
    "    gp.fit(X_initial, y_initial)\n",
    "    predictions, std_dev = gp.predict(X_pool, return_std=True)\n",
    "    uncertainty_idx = np.argsort(std_dev)[-n_to_add_each_iter:]\n",
    "    X_initial = np.vstack([X_initial, X_pool[uncertainty_idx]])\n",
    "    y_initial = np.hstack([y_initial, y_pool[uncertainty_idx]])\n",
    "    X_pool = np.delete(X_pool, uncertainty_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, uncertainty_idx)\n",
    "\n",
    "    # New code: Record the model's performance on the test set\n",
    "    y_pred_test = gp.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    mse_test.append(mse)\n",
    "\n",
    "    print(f\"Iteration {i+1}/{n_iterations} complete. Training set size: {len(y_initial)} | Test MSE: {mse:.4f}\")\n",
    "\n",
    "# After the loop, visualize the model's performance over iterations\n",
    "plt.plot(range(n_iterations), mse_test, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error on Test Set\")\n",
    "plt.title(\"Model Performance Over Iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bfa454df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_iterations = 10  # You can change this to any desired number\\nn_to_add_each_iter = 100  # Number of points to add from the pool to training set in each iteration\\n\\nfor i in range(n_iterations):\\n    # Train Gaussian Process on current training data\\n    gp.fit(X_initial, y_initial)\\n\\n    # Predict on the unlabeled pool\\n    predictions, std_dev = gp.predict(X_pool, return_std=True)\\n\\n    # Select points where the model is least confident (highest standard deviation)\\n    uncertainty_idx = np.argsort(std_dev)[-n_to_add_each_iter:]\\n\\n    # Append the uncertain points to the training data\\n    X_initial = np.vstack([X_initial, X_pool[uncertainty_idx]])\\n    y_initial = np.hstack([y_initial, y_pool[uncertainty_idx]])\\n\\n    # Remove the uncertain points from the pool\\n    X_pool = np.delete(X_pool, uncertainty_idx, axis=0)\\n    y_pool = np.delete(y_pool, uncertainty_idx)\\n\\n    # Optionally, print progress\\n    print(f\"Iteration {i+1}/{n_iterations} complete. Training set size: {len(y_initial)}\")\\n\\nprint(\"Active learning process complete!\")\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "n_iterations = 10  # You can change this to any desired number\n",
    "n_to_add_each_iter = 100  # Number of points to add from the pool to training set in each iteration\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Train Gaussian Process on current training data\n",
    "    gp.fit(X_initial, y_initial)\n",
    "\n",
    "    # Predict on the unlabeled pool\n",
    "    predictions, std_dev = gp.predict(X_pool, return_std=True)\n",
    "\n",
    "    # Select points where the model is least confident (highest standard deviation)\n",
    "    uncertainty_idx = np.argsort(std_dev)[-n_to_add_each_iter:]\n",
    "\n",
    "    # Append the uncertain points to the training data\n",
    "    X_initial = np.vstack([X_initial, X_pool[uncertainty_idx]])\n",
    "    y_initial = np.hstack([y_initial, y_pool[uncertainty_idx]])\n",
    "\n",
    "    # Remove the uncertain points from the pool\n",
    "    X_pool = np.delete(X_pool, uncertainty_idx, axis=0)\n",
    "    y_pool = np.delete(y_pool, uncertainty_idx)\n",
    "\n",
    "    # Optionally, print progress\n",
    "    print(f\"Iteration {i+1}/{n_iterations} complete. Training set size: {len(y_initial)}\")\n",
    "\n",
    "print(\"Active learning process complete!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd9263be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Hydrogenation Enthalpy (kJ/mol H2) for the new molecule: 45.67651955256494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hassan/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cc1cc2ccccc2o1\n",
    "\n",
    "new_molecule_smiles = \"c1cnc2ccc(C3CC3)cc2c1\"  # Replace with your SMILES string\n",
    "\n",
    "# Convert to RDKit mol object\n",
    "new_mol = Chem.MolFromSmiles(new_molecule_smiles)\n",
    "\n",
    "# Check if the conversion is successful\n",
    "if new_mol:\n",
    "    new_molecule_descriptors = evaluate_chem_mol(new_mol)\n",
    "else:\n",
    "    raise ValueError(\"Invalid SMILES string or molecule conversion failed.\")\n",
    "\n",
    "# Convert descriptors to a numpy array and reshape for single sample prediction\n",
    "new_molecule_descriptors = np.array(new_molecule_descriptors).reshape(1, -1)\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# List of descriptor columns (you've defined these when you computed descriptors)\n",
    "descriptor_cols = [\n",
    "    'mol_sssr', 'clogp', 'mr', 'mw', 'tpsa', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n', \n",
    "    'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v', 'fracsp3', 'Hall_Kier_Alpha','Kappa1', \n",
    "    'Kappa2', 'Kappa3', 'LabuteASA', 'Number_Aliphatic_Rings', 'Number_Aromatic_Rings', \n",
    "    'Number_Amide_Bonds', 'Number_Atom_Stereocenters', 'Number_BridgeHead_Atoms', 'Number_HBA', \n",
    "    'Number_HBD', 'Number_Hetero_Atoms', 'Number_Hetero_Cycles', 'Number_Rings', 'Number_Rotatable_Bonds', \n",
    "    'Number_Spiro', 'Number_Saturated_Rings', 'Number_Heavy_Atoms', 'Number_NH_OH', 'Number_N_O', \n",
    "    'Number_Valence_Electrons', 'Max_Partial_Charge', 'Min_Partial_Charge'\n",
    "]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data descriptors\n",
    "scaler.fit(train_data[descriptor_cols])\n",
    "\n",
    "new_molecule_descriptors_scaled = scaler.transform(new_molecule_descriptors)\n",
    "\n",
    "\n",
    "predicted_delta_H = gp.predict(new_molecule_descriptors_scaled)\n",
    "print(f\"Predicted Hydrogenation Enthalpy (kJ/mol H2) for the new molecule: {predicted_delta_H[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279bd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5eb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12e138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4de7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
